{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"moonDataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eabda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows_count = df.duplicated().sum()\n",
    "print('Duplicated rows (row by row):', duplicated_rows_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ea2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    \"X1\": \"First moons feature\",\n",
    "    \"X2\": \"Second moons feature\",\n",
    "    \"X3\": \"Vertical displacement\",\n",
    "    \"label\": \"class\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41322383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df[\"class\"].map({0: \"No\", 1: \"Yes\"})\n",
    "custom_palette = {\"Yes\": \"salmon\", \"No\": \"skyblue\"}\n",
    "\n",
    "num_cols = [\n",
    "    \"First moons feature\",\n",
    "    \"Second moons feature\",\n",
    "    \"Vertical displacement\"\n",
    "]\n",
    "cat_col = \"class\"\n",
    "\n",
    "fig, axes = plt.subplots(len(num_cols), 4, figsize=(28, 36))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(df[col], kde=True, ax=axes[i, 0], bins=10, color=\"lightblue\")\n",
    "    axes[i, 0].axvline(\n",
    "        df[col].mean(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean {col}: {df[col].mean():.2f}\",\n",
    "    )\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 0].set_title(f\"Histogram of {col} with Mean\")\n",
    "\n",
    "    sns.boxplot(x=df[col], ax=axes[i, 1], color=\"lightblue\")\n",
    "    axes[i, 1].set_title(f\"Boxplot of {col}\")\n",
    "\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=col,\n",
    "        hue=cat_col,\n",
    "        kde=True,\n",
    "        ax=axes[i, 2],\n",
    "        bins=10,\n",
    "        palette=custom_palette,\n",
    "    )\n",
    "    \n",
    "    for grp, color in zip([\"No\", \"Yes\"], [\"blue\", \"red\"]):\n",
    "        if grp in df[cat_col].unique():\n",
    "            axes[i, 2].axvline(\n",
    "                df[df[cat_col] == grp][col].mean(),\n",
    "                color=color,\n",
    "                linestyle=\"--\",\n",
    "                label=f\"Mean {col} ({grp}): {df[df[cat_col] == grp][col].mean():.2f}\",\n",
    "            )\n",
    "    axes[i, 2].legend()\n",
    "    axes[i, 2].set_title(f\"Histogram of {col} by {cat_col}\")\n",
    "\n",
    "    sns.boxplot(x=cat_col, y=col, hue=cat_col, data=df, ax=axes[i, 3], palette=custom_palette, legend=False)\n",
    "    axes[i, 3].set_title(f\"Boxplot of {col} by {cat_col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75882a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df[\"class\"].map({\"No\": 0, \"Yes\": 1})\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "num_features = len(correlation_matrix)\n",
    "font_size = max(5, 40 // num_features)\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    fmt=\".2f\",\n",
    "    cbar=True,\n",
    "    annot_kws={\"size\": font_size},\n",
    ")\n",
    "\n",
    "plt.title(\"Correlation Matrix\", fontsize=20)\n",
    "plt.xticks(fontsize=font_size)\n",
    "plt.yticks(fontsize=font_size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, train_size=0.6, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values if hasattr(X, 'values') else X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values if hasattr(y, 'values') else y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "test_dataset = TabularDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04419a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_accuracy = evaluate(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f} - Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(y_batch.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ece38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = ClassificationModel(input_dim, output_dim)\n",
    "train_model(model, train_loader, val_loader, epochs=30, lr=0.001)\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(y_batch.tolist())\n",
    "        y_pred.extend(predicted.tolist())\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_np = X_test.values if hasattr(X_test, 'values') else X_test\n",
    "y_test_np = y_test if isinstance(y_test, np.ndarray) else np.array(y_test)\n",
    "\n",
    "f0_min, f0_max = X_test_np[:, 0].min() - 0.1, X_test_np[:, 0].max() + 0.1\n",
    "f1_min, f1_max = X_test_np[:, 1].min() - 0.1, X_test_np[:, 1].max() + 0.1\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(f0_min, f0_max, 300),\n",
    "                     np.linspace(f1_min, f1_max, 300))\n",
    "\n",
    "fixed_feature2 = np.mean(X_test_np[:, 2])\n",
    "\n",
    "grid_input = np.c_[\n",
    "    xx.ravel(),\n",
    "    yy.ravel(),\n",
    "    np.full(xx.ravel().shape, fixed_feature2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    grid_tensor = torch.tensor(grid_input, dtype=torch.float32)\n",
    "    outputs = model(grid_tensor)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    Z = predictions.numpy().reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap='Pastel1', alpha=0.6)\n",
    "\n",
    "scatter = plt.scatter(X_test_np[:, 0], X_test_np[:, 1], c=y_test_np, cmap='tab10', edgecolor='k')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.title(\"Decision Boundaries (Feature 2 fixed at mean)\")\n",
    "\n",
    "plt.legend(*scatter.legend_elements(), title=\"True Class\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eef098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445dc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['First moons feature', 'Second moons feature', 'Vertical displacement']\n",
    "target = 'class'\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[target])\n",
    "\n",
    "dls = TabularDataLoaders.from_df(\n",
    "    df=pd.concat([train_df, valid_df]),\n",
    "    path='.',\n",
    "    procs=[Normalize],\n",
    "    cat_names=[],\n",
    "    cont_names=features,\n",
    "    y_names=target,\n",
    "    y_block=CategoryBlock(),\n",
    "    valid_idx=list(range(len(train_df), len(train_df) + len(valid_df))),\n",
    "    bs=16\n",
    ")\n",
    "\n",
    "learn = tabular_learner(\n",
    "    dls,\n",
    "    layers=[256, 128, 64, 32],\n",
    "    config=tabular_config(ps=[0.4, 0.3, 0.2, 0.1]),\n",
    "    metrics=accuracy\n",
    ")\n",
    "\n",
    "lr_min = learn.lr_find().valley\n",
    "learn.fit_one_cycle(20, lr_max=lr_min)\n",
    "\n",
    "preds, targs = learn.get_preds()\n",
    "pred_labels = preds.argmax(dim=1)\n",
    "\n",
    "print(confusion_matrix(targs, pred_labels))\n",
    "print(classification_report(targs, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['First moons feature', 'Second moons feature', 'Vertical displacement']\n",
    "f0, f1 = features[0], features[1]\n",
    "fixed_f2 = df['Vertical displacement'].mean()\n",
    "\n",
    "x_min, x_max = df[f0].min() - 0.1, df[f0].max() + 0.1\n",
    "y_min, y_max = df[f1].min() - 0.1, df[f1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                     np.linspace(y_min, y_max, 300))\n",
    "\n",
    "grid = np.c_[\n",
    "    xx.ravel(),\n",
    "    yy.ravel(),\n",
    "    np.full(xx.ravel().shape, fixed_f2)\n",
    "]\n",
    "grid_df = pd.DataFrame(grid, columns=features)\n",
    "\n",
    "test_dl = learn.dls.test_dl(grid_df)\n",
    "preds, _ = learn.get_preds(dl=test_dl)\n",
    "Z = preds.argmax(dim=1).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, cmap='Pastel1', alpha=0.6)\n",
    "\n",
    "scatter = plt.scatter(df[f0], df[f1], c=df['class'], cmap='tab10', edgecolor='k')\n",
    "plt.xlabel(f0)\n",
    "plt.ylabel(f1)\n",
    "plt.title(\"Decision Boundary (Fastai Learner)\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"True Class\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702545bb",
   "metadata": {},
   "source": [
    "# PP5/6 Deep Learning on a Non-Linearly Separable Moons Dataset\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project explores deep learning for binary classification using a **non-linearly separable dataset**. The dataset was designed to simulate a challenging decision boundary and was modeled using **two deep learning frameworks**: **PyTorch** and **fastai**.\n",
    "\n",
    "The main objective was to train a neural network to distinguish between two classes using three non-linearly correlated features. The task includes full data preprocessing, visualization, neural network implementation and training, evaluation, and visual confirmation of decision boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset used is based on the classic `make_moons` structure, extended with a third feature to introduce further vertical displacement. It consists of the following columns:\n",
    "\n",
    "- **X1**: First moons feature  \n",
    "- **X2**: Second moons feature  \n",
    "- **X3**: Vertical displacement (added noise/offset to increase complexity)  \n",
    "- **label**: Target class (0 or 1), where 1 indicates \"Yes\" and 0 indicates \"No\"\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "- 3 numerical input features  \n",
    "- Binary classification target  \n",
    "- Non-linear decision boundary  \n",
    "- 1,000+ samples with no duplicates  \n",
    "- Well-balanced class distribution\n",
    "\n",
    "The dataset was thoroughly analyzed using boxplots, histograms, and a correlation matrix to understand the relationships between features and to confirm class separability is non-trivial.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Two separate deep learning pipelines were built:\n",
    "\n",
    "### 1. PyTorch\n",
    "- Manual dataset and dataloader creation\n",
    "- Custom `nn.Module` with 3 hidden layers (ReLU activations)\n",
    "- Binary cross-entropy loss (`BCELoss`)\n",
    "- Trained using Adam optimizer\n",
    "- Model performance monitored via accuracy and confusion matrix\n",
    "- 3D decision surface plotted using `matplotlib`\n",
    "\n",
    "### 2. fastai\n",
    "- Used `TabularPandas` to preprocess and encode data\n",
    "- Created a learner with `CrossEntropyLossFlat`\n",
    "- Trained with built-in training loop (`fit_one_cycle`)\n",
    "- Visualized classification performance and generated predictions\n",
    "\n",
    "Both frameworks followed the same data split:  \n",
    "- 60% training  \n",
    "- 20% validation  \n",
    "- 20% testing\n",
    "\n",
    "---\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "- The dataset poses a non-trivial classification challenge due to the curved and overlapping feature space.\n",
    "- Both models achieved high accuracy (>95%) on the test set.\n",
    "- PyTorch provided more control and transparency during training, especially for manual inspection of weights and predictions.\n",
    "- fastai enabled rapid prototyping with minimal code and auto-handled much of the boilerplate preprocessing and training logic.\n",
    "- Visual decision boundaries showed clear learned separation even in the presence of noise.\n",
    "\n",
    "---\n",
    "\n",
    "## Framework Comparison\n",
    "\n",
    "Between the two frameworks, **fastai** was the most efficient for quick experimentation and high-level abstraction. Its API allowed for compact, readable code and fast training. I especially appreciated how fastai handled data preprocessing, splitting, and training loops with just a few lines of code, making it ideal for rapid iteration and model comparison.\n",
    "\n",
    "On the other hand, **PyTorch** gave full control over model architecture, weight updates, and custom metrics. It was an excellent framework for learning and debugging deep learning logic from the ground up. It required more code, but it made every training step explicit and transparent.\n",
    "\n",
    "That said, for real-world tasks where productivity and speed matter, I found **fastai** to be the better choice overall—especially for tabular or structured data classification tasks like this one. However, when deep customization or flexibility is required, **PyTorch** remains unmatched. Each framework has its place, and using both has helped me appreciate their strengths in different scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "\n",
    "**requirements.txt** :\n",
    "```\n",
    "python == 3.10.18\n",
    "pandas == 2.3.1\n",
    "matplotlib == 3.10.0\n",
    "seaborn == 0.13.2\n",
    "numpy == 1.26.4\n",
    "scikit-learn == 1.7.1\n",
    "pytorch == 2.2.2\n",
    "fastai == 2.7.17\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd6ea47",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
